<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        var scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          var element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          var hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          var newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((entries) => {
          setHeight();
        });
        resizeObserver.observe(obj.contentWindow.document.body);
      }
    </script>
    <marimo-filename hidden>presentation.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>Concrete Numeric Representations in LLM Embeddings</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/index-DgI7bmFZ.js"></script>
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.9/dist/assets/index-B-hr6ABS.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "presentation.py",
            "mode": "read",
            "version": "0.14.9",
            "serverToken": "static",
            "config": {"completion": {"activate_on_typing": true, "copilot": false}, "display": {"cell_output": "above", "code_editor_font_size": 14, "dataframes": "rich", "default_table_page_size": 10, "default_width": "medium", "reference_highlighting": false, "theme": "dark"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": true}}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": true, "auto_reload": "off", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"app_title": "Concrete Numeric Representations in LLM Embeddings", "layout_file": "layouts/presentation.slides.json", "sql_output": "auto", "width": "compact"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "import marimo as mo", "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "setup", "name": "setup"}, {"code": "mo.md(\n    r\"\"\"\n# Concrete Numeric Representations in LLM Embeddings\n\n## Gentiletti Emanuele\n## Unito 2024-2025\n\"\"\"\n)", "code_hash": "0d76a673aceb3682cf63c05f22090099", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "CCZx", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# What is a Concrete Representation?\n\n- The hypothesis we explore is the presence of geometric structures in the organization of numerical embeddings that facilitates mathematical computations.\n\n- Precedent in humans: **Savant Syndrome**\n    - Some people are able to perform extraordinary feats of mathematical prowess with little effort.\n    - It is proposed in the scientific literature that this ability doesn't come through algorithmic processes, but through the consultation of encoded geometric structures that reveal the answer\n        - **Concrete Representations** according to Murray et al., 2005 <?>\n    - Sequence-space synesthesia\n        - Savants and Synasthetes are able to visualize sequences in space and get answers to mathematical questions just by navigating them in space\n\"\"\"\n)", "code_hash": "8b29ef93688f8e8f6db4a5bceedc8946", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "gSOO", "name": "_"}, {"code": "mo.md(\n    rf\"\"\"\n# LLM Embeddings\n\n!!!\n\n- String is split in tokens, each token corresponds to a single vector of weights in the first layer\n- Embeddings are trained through gradient descent, which changes the weights to minimize a target error function\n\n![Embeddings example](\"public/embeddings_example.png)\n\"\"\"\n)", "code_hash": "234b54215ff5ffca102340d90d618146", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iNSP", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# Bad tokenization schemes\n\n- Most current LLMs tokenize numbers in an unintuitive way.\n    - L2R  tokenization: $\\underbrace{123} \\; \\underbrace{456} \\; \\underbrace{7}$\n    - R2L  tokenization: $\\underbrace{1} \\; \\underbrace{234} \\; \\underbrace{567}$\n- R2L is preferrable for numeric computations (think doing additions in column), so why do LLMs use L2R?\n    - Reason: BPE algorithm\n\n[\ud83d\udd17 Tiktokenizer](https://tiktokenizer.vercel.app/)\n\"\"\"\n)", "code_hash": "210a2976bc05858b0855544279db27d3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Dwpm", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# BPE algorithm\n- We construct a list of tokens for the LLM to consider as \"units\" to work with\n    - Start: each singular character gets a token (`a`, `b`, `c`, ...)\n    - Until we reach the target `vocabulary_size`:\n          - Add the most common occurring pair of tokens as a new token\n              - For example, the common preposition `to` is very common in English text, so it may get added to the target vocabulary (`a`, `b`, `c`, ..., `to`)\n              - After some cycles, the most common occorring pair of tokens might be `to` and `m` to form the common name `tom` (`a`, `b`, `c`, ..., `to`, ..., `tom`)\n              - This is how vocabularies in modern LLMs are built.\n\n  \n\"\"\"\n)", "code_hash": "0d95420336863fe0bda86954da0784d4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "pzNE", "name": "_"}, {"code": "mo.md(\n    rf\"\"\"\n- BPE constructs its vocabulary by associating tokens from left to right.\n- Tokenizers split sentences according to the vocabulary with the same logic (L2R).\n- GPT-2 and GPT-3 used the same criteria for numbers as it did for words, leading to tokenization of only most frequently occurring numbers !!!\n- Most open-source LLMs today always tokenize integers from 0 to 999 using a single token, but they still group tokens L2R (123,456,7)\n![GPT-2 unique numeric tokens. A yellow square means the corresponding number is represented with a single token.](\"public/unique_tokens.png\")\n[\ud83d\udd17 beren.io - Integer Tokenization is Insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n\"\"\"\n)", "code_hash": "5c83a26e91bbc395a0fb4b67d1b1e0f4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Tjhr", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# Bad Clustering and Inference-time correction\n\n- A study has been conducted to benchmark addition performed with both L2R and R2L clustering\n      - \ud83e\uddea Experiment: ask the same addition problems twice\n          - First time neutrally: 1234567 + 654321\n          - Second time clustering digits with commas: 1,234,567 + 654,321\n              - Commas force token separation at the point of insertion\n- R2L tokenization dramatically outperforms standard L2R tokenization\n    - GPT-3.5: 75.6% accuracy (L2R) vs 97.8% accuracy (R2L)\n    - GPT-4: 84.4% accuracy (L2R) vs 98.9% accuracy (R2L)\n  \n\n[\ud83d\udcda Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/abs/2402.14903)\n\"\"\"\n)", "code_hash": "38f23bc9091f6bfc1fc7579cf60b3169", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "abwc", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n- The models had been trained with a L2R tokenization scheme, yet forcing a R2L one improves performance without needing to train the model again\n\n-  The model weights appear to have learned arithmetic algorithms that are fundamentally R2L-oriented, despite being trained predominantly on L2R tokenized data.\n    - Inductive biases emerge from mathematical structure, not just data statistics <?>\n\n- This is also a clue that the **underlying embedding representation of numbers is representative of general mathematical principles**, and not just rote memorization.\n\n\"\"\"\n)", "code_hash": "de7dbc5c22241486ea401cc177610148", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "IFpf", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# The Platonic Representation Hypothesis\n\n[\ud83d\udcdaHuh et al. (2024)](https://arxiv.org/abs/2405.07987) argue that all models are converging to a shared statistical model of reality.\n\n- **Vision-Vision Alignment**\n    - Tested 78 vision models of various architectures and training objectives\n    - Models with higher performance on VTAB tasks showed greater mutual alignment\n    - Competent models clustered together in representation space\n\n- **Cross-Modal Alignment**\n    - Vision models and language models show increasing alignment as they scale\n    - Models can be \"stitched\" together across modalities with simple learned mappings\n    - Color representations learned from text match those learned from images\n\n- **Evidence from Model Stitching**\n    - Different models can have their intermediate layers successfully swapped\n    - This works even across different training objectives and datasets\n    - Success indicates compatible internal representations\n\n\"\"\"\n)", "code_hash": "4d7f1c65b0812744ac4c5fd1ba596d5d", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lscN", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\nTo recap:\n\n- Concrete structures can provide Savants and synaesthetes access to mathematical knowledge\n- LLMs generalize over mathematical concepts even when they're trained wrongly\n- Representations in LLMs seem to be converging\n\nBy looking into the structure of embeddings, we seek to understand the shape these might approach. To understand them, we represent them using dimensionality reduction techniques.\n\n\"\"\"\n)", "code_hash": "a2e8dec6a1954f09923b7ea07361fdcb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "FBXJ", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# Dimensionality Reduction\nWe employ the most commonly used dimensionality reduction techniques:\n\n- **Linear**\n    - **Singular Value Decomposition**  \n        - decomposes a matrix as $A = U \\Sigma V^T$ (rotation, stretch, rotation). $\\Sigma$ contains the singular values, which **measure the data stretch along each principal direction**.  \n          - We then sort the features by their singular values to get the most significant ones.\n      \n    - **Principal Component Analysis**  \n          - like SVD, but centers the features (subtracts the mean) first. As a consequence, the singular values correspond to **the standard deviations along each principal component**.\n          - has cleaner statistical interpretation, but can destroy non-centered structures\n\n- **Non-Linear**\n    - **t-SNE**  \n          - Models proximity relationships in the data as a probability distribution\n          - Recreates the data points in a lower-dimensional space, optimizing them through gradient descent to have the same distribution\n    - **UMAP**\n        - preserves both local and global structure using topological data analysis. Faster than t-SNE with better scalability.\n        - more stable results, better preservation of global relationships\n\"\"\"\n)", "code_hash": "98c27d9ab48608e9f5b4411588b21dcb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "krMH", "name": "_"}, {"code": "mo.md(\n    r\"\"\"\n# Number Semantics\n\n- We also have an opportunity to check semantics in a way that crosses the symbolic barrier.\n- We can relate the symbols themselves (0, 1, 2, ..., 999) to the **numeric components that constitute the embeddings**\n- We explore this idea by checking individual features for all the integer embeddings and  their correlation with important mathematical sequences:\n    - $n_i = i$ (the numbers themselves)\n    - $n_i = \\log(i)$\n    - Prime numbers\n    - Fibonacci numbers\n    - Triangular numbers\n\"\"\"\n)", "code_hash": "e3260b62a20508d95cd2009acbcbd8b8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RGno", "name": "_"}, {"code": "", "code_hash": null, "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dNNg", "name": "_"}], "metadata": {"marimo_version": "0.14.9"}, "version": "1"},
            "session": {"cells": [{"code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "console": [], "id": "setup", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "0d76a673aceb3682cf63c05f22090099", "console": [], "id": "CCZx", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"concrete-numeric-representations-in-llm-embeddings\">Concrete Numeric Representations in LLM Embeddings</h1>\n<h2 id=\"gentiletti-emanuele\">Gentiletti Emanuele</h2>\n<h2 id=\"unito-2024-2025\">Unito 2024-2025</h2></span>"}, "type": "data"}]}, {"code_hash": "8b29ef93688f8e8f6db4a5bceedc8946", "console": [], "id": "gSOO", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"what-is-a-concrete-representation\">What is a Concrete Representation?</h1>\n<ul>\n<li>\n<span class=\"paragraph\">The hypothesis we explore is the presence of geometric structures in the organization of numerical embeddings that facilitates mathematical computations.</span>\n</li>\n<li>\n<span class=\"paragraph\">Precedent in humans: <strong>Savant Syndrome</strong></span>\n<ul>\n<li>Some people are able to perform extraordinary feats of mathematical prowess with little effort.</li>\n<li>It is proposed in the scientific literature that this ability doesn't come through algorithmic processes, but through the consultation of encoded geometric structures that reveal the answer<ul>\n<li><strong>Concrete Representations</strong> according to Murray et al., 2005 &lt;?&gt;</li>\n</ul>\n</li>\n<li>Sequence-space synesthesia<ul>\n<li>Savants and Synasthetes are able to visualize sequences in space and get answers to mathematical questions just by navigating them in space</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "234b54215ff5ffca102340d90d618146", "console": [], "id": "iNSP", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"llm-embeddings\">LLM Embeddings</h1>\n<span class=\"paragraph\">!!!</span>\n<ul>\n<li>String is split in tokens, each token corresponds to a single vector of weights in the first layer</li>\n<li>Embeddings are trained through gradient descent, which changes the weights to minimize a target error function</li>\n</ul>\n<span class=\"paragraph\"><img alt=\"Embeddings example\" src=\"&quot;public/embeddings_example.png\" /></span></span>"}, "type": "data"}]}, {"code_hash": "210a2976bc05858b0855544279db27d3", "console": [], "id": "Dwpm", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"bad-tokenization-schemes\">Bad tokenization schemes</h1>\n<ul>\n<li>Most current LLMs tokenize numbers in an unintuitive way.<ul>\n<li>L2R  tokenization: <marimo-tex class=\"arithmatex\">||(\\underbrace{123} \\; \\underbrace{456} \\; \\underbrace{7}||)</marimo-tex></li>\n<li>R2L  tokenization: <marimo-tex class=\"arithmatex\">||(\\underbrace{1} \\; \\underbrace{234} \\; \\underbrace{567}||)</marimo-tex></li>\n</ul>\n</li>\n<li>R2L is preferrable for numeric computations (think doing additions in column), so why do LLMs use L2R?<ul>\n<li>Reason: BPE algorithm</li>\n</ul>\n</li>\n</ul>\n<span class=\"paragraph\"><a href=\"https://tiktokenizer.vercel.app/\" rel=\"noopener\" target=\"_blank\">\ud83d\udd17 Tiktokenizer</a></span></span>"}, "type": "data"}]}, {"code_hash": "0d95420336863fe0bda86954da0784d4", "console": [], "id": "pzNE", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"bpe-algorithm\">BPE algorithm</h1>\n<ul>\n<li>We construct a list of tokens for the LLM to consider as \"units\" to work with<ul>\n<li>Start: each singular character gets a token (<code>a</code>, <code>b</code>, <code>c</code>, ...)</li>\n<li>Until we reach the target <code>vocabulary_size</code>:<ul>\n<li>Add the most common occurring pair of tokens as a new token<ul>\n<li>For example, the common preposition <code>to</code> is very common in English text, so it may get added to the target vocabulary (<code>a</code>, <code>b</code>, <code>c</code>, ..., <code>to</code>)</li>\n<li>After some cycles, the most common occorring pair of tokens might be <code>to</code> and <code>m</code> to form the common name <code>tom</code> (<code>a</code>, <code>b</code>, <code>c</code>, ..., <code>to</code>, ..., <code>tom</code>)</li>\n<li>This is how vocabularies in modern LLMs are built.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "5c83a26e91bbc395a0fb4b67d1b1e0f4", "console": [], "id": "Tjhr", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><ul>\n<li>BPE constructs its vocabulary by associating tokens from left to right.</li>\n<li>Tokenizers split sentences according to the vocabulary with the same logic (L2R).</li>\n<li>GPT-2 and GPT-3 used the same criteria for numbers as it did for words, leading to tokenization of only most frequently occurring numbers !!!</li>\n<li>Most open-source LLMs today always tokenize integers from 0 to 999 using a single token, but they still group tokens L2R (123,456,7)\n<img alt=\"GPT-2 unique numeric tokens. A yellow square means the corresponding number is represented with a single token.\" src=\"\" title=\"public/unique_tokens.png\" />\n<a href=\"https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/\" rel=\"noopener\" target=\"_blank\">\ud83d\udd17 beren.io - Integer Tokenization is Insane</a></li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "38f23bc9091f6bfc1fc7579cf60b3169", "console": [], "id": "abwc", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"bad-clustering-and-inference-time-correction\">Bad Clustering and Inference-time correction</h1>\n<ul>\n<li>A study has been conducted to benchmark addition performed with both L2R and R2L clustering<ul>\n<li>\ud83e\uddea Experiment: ask the same addition problems twice<ul>\n<li>First time neutrally: 1234567 + 654321</li>\n<li>Second time clustering digits with commas: 1,234,567 + 654,321<ul>\n<li>Commas force token separation at the point of insertion</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>R2L tokenization dramatically outperforms standard L2R tokenization<ul>\n<li>GPT-3.5: 75.6% accuracy (L2R) vs 97.8% accuracy (R2L)</li>\n<li>GPT-4: 84.4% accuracy (L2R) vs 98.9% accuracy (R2L)</li>\n</ul>\n</li>\n</ul>\n<span class=\"paragraph\"><a href=\"https://arxiv.org/abs/2402.14903\" rel=\"noopener\" target=\"_blank\">\ud83d\udcda Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</a></span></span>"}, "type": "data"}]}, {"code_hash": "de7dbc5c22241486ea401cc177610148", "console": [], "id": "IFpf", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><ul>\n<li>\n<span class=\"paragraph\">The models had been trained with a L2R tokenization scheme, yet forcing a R2L one improves performance without needing to train the model again</span>\n</li>\n<li>\n<span class=\"paragraph\">The model weights appear to have learned arithmetic algorithms that are fundamentally R2L-oriented, despite being trained predominantly on L2R tokenized data.</span>\n<ul>\n<li>Inductive biases emerge from mathematical structure, not just data statistics &lt;?&gt;</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\">This is also a clue that the <strong>underlying embedding representation of numbers is representative of general mathematical principles</strong>, and not just rote memorization.</span>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "4d7f1c65b0812744ac4c5fd1ba596d5d", "console": [], "id": "lscN", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"the-platonic-representation-hypothesis\">The Platonic Representation Hypothesis</h1>\n<span class=\"paragraph\"><a href=\"https://arxiv.org/abs/2405.07987\" rel=\"noopener\" target=\"_blank\">\ud83d\udcdaHuh et al. (2024)</a> argue that all models are converging to a shared statistical model of reality.</span>\n<ul>\n<li>\n<span class=\"paragraph\"><strong>Vision-Vision Alignment</strong></span>\n<ul>\n<li>Tested 78 vision models of various architectures and training objectives</li>\n<li>Models with higher performance on VTAB tasks showed greater mutual alignment</li>\n<li>Competent models clustered together in representation space</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Cross-Modal Alignment</strong></span>\n<ul>\n<li>Vision models and language models show increasing alignment as they scale</li>\n<li>Models can be \"stitched\" together across modalities with simple learned mappings</li>\n<li>Color representations learned from text match those learned from images</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Evidence from Model Stitching</strong></span>\n<ul>\n<li>Different models can have their intermediate layers successfully swapped</li>\n<li>This works even across different training objectives and datasets</li>\n<li>Success indicates compatible internal representations</li>\n</ul>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "a2e8dec6a1954f09923b7ea07361fdcb", "console": [], "id": "FBXJ", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">To recap:</span>\n<ul>\n<li>Concrete structures can provide Savants and synaesthetes access to mathematical knowledge</li>\n<li>LLMs generalize over mathematical concepts even when they're trained wrongly</li>\n<li>Representations in LLMs seem to be converging</li>\n</ul>\n<span class=\"paragraph\">By looking into the structure of embeddings, we seek to understand the shape these might approach. To understand them, we represent them using dimensionality reduction techniques.</span></span>"}, "type": "data"}]}, {"code_hash": "98c27d9ab48608e9f5b4411588b21dcb", "console": [], "id": "krMH", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"dimensionality-reduction\">Dimensionality Reduction</h1>\n<span class=\"paragraph\">We employ the most commonly used dimensionality reduction techniques:</span>\n<ul>\n<li>\n<span class=\"paragraph\"><strong>Linear</strong></span>\n<ul>\n<li>\n<span class=\"paragraph\"><strong>Singular Value Decomposition</strong>  </span>\n<ul>\n<li>decomposes a matrix as <marimo-tex class=\"arithmatex\">||(A = U \\Sigma V^T||)</marimo-tex> (rotation, stretch, rotation). <marimo-tex class=\"arithmatex\">||(\\Sigma||)</marimo-tex> contains the singular values, which <strong>measure the data stretch along each principal direction</strong>.  </li>\n<li>We then sort the features by their singular values to get the most significant ones.</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Principal Component Analysis</strong>  </span>\n<ul>\n<li>like SVD, but centers the features (subtracts the mean) first. As a consequence, the singular values correspond to <strong>the standard deviations along each principal component</strong>.</li>\n<li>has cleaner statistical interpretation, but can destroy non-centered structures</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Non-Linear</strong></span>\n<ul>\n<li><strong>t-SNE</strong>  <ul>\n<li>Models proximity relationships in the data as a probability distribution</li>\n<li>Recreates the data points in a lower-dimensional space, optimizing them through gradient descent to have the same distribution</li>\n</ul>\n</li>\n<li><strong>UMAP</strong><ul>\n<li>preserves both local and global structure using topological data analysis. Faster than t-SNE with better scalability.</li>\n<li>more stable results, better preservation of global relationships</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": "e3260b62a20508d95cd2009acbcbd8b8", "console": [], "id": "RGno", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"number-semantics\">Number Semantics</h1>\n<ul>\n<li>We also have an opportunity to check semantics in a way that crosses the symbolic barrier.</li>\n<li>We can relate the symbols themselves (0, 1, 2, ..., 999) to the <strong>numeric components that constitute the embeddings</strong></li>\n<li>We explore this idea by checking individual features for all the integer embeddings and  their correlation with important mathematical sequences:<ul>\n<li><marimo-tex class=\"arithmatex\">||(n_i = i||)</marimo-tex> (the numbers themselves)</li>\n<li><marimo-tex class=\"arithmatex\">||(n_i = \\log(i)||)</marimo-tex></li>\n<li>Prime numbers</li>\n<li>Fibonacci numbers</li>\n<li>Triangular numbers</li>\n</ul>\n</li>\n</ul></span>"}, "type": "data"}]}, {"code_hash": null, "console": [], "id": "dNNg", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}], "metadata": {"marimo_version": "0.14.9"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    import%20marimo%0A%0A__generated_with%20%3D%20%220.14.9%22%0Aapp%20%3D%20marimo.App(%0A%20%20%20%20app_title%3D%22Concrete%20Numeric%20Representations%20in%20LLM%20Embeddings%22%2C%0A%20%20%20%20layout_file%3D%22layouts%2Fpresentation.slides.json%22%2C%0A)%0A%0Awith%20app.setup%3A%0A%20%20%20%20import%20marimo%20as%20mo%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20Concrete%20Numeric%20Representations%20in%20LLM%20Embeddings%0A%0A%20%20%20%20%23%23%20Gentiletti%20Emanuele%0A%20%20%20%20%23%23%20Unito%202024-2025%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20What%20is%20a%20Concrete%20Representation%3F%0A%0A%20%20%20%20-%20The%20hypothesis%20we%20explore%20is%20the%20presence%20of%20geometric%20structures%20in%20the%20organization%20of%20numerical%20embeddings%20that%20facilitates%20mathematical%20computations.%0A%0A%20%20%20%20-%20Precedent%20in%20humans%3A%20**Savant%20Syndrome**%0A%20%20%20%20%20%20%20%20-%20Some%20people%20are%20able%20to%20perform%20extraordinary%20feats%20of%20mathematical%20prowess%20with%20little%20effort.%0A%20%20%20%20%20%20%20%20-%20It%20is%20proposed%20in%20the%20scientific%20literature%20that%20this%20ability%20doesn't%20come%20through%20algorithmic%20processes%2C%20but%20through%20the%20consultation%20of%20encoded%20geometric%20structures%20that%20reveal%20the%20answer%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**Concrete%20Representations**%20according%20to%20Murray%20et%20al.%2C%202005%20%3C%3F%3E%0A%20%20%20%20%20%20%20%20-%20Sequence-space%20synesthesia%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20Savants%20and%20Synasthetes%20are%20able%20to%20visualize%20sequences%20in%20space%20and%20get%20answers%20to%20mathematical%20questions%20just%20by%20navigating%20them%20in%20space%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20LLM%20Embeddings%0A%0A%20%20%20%20!!!%0A%0A%20%20%20%20-%20String%20is%20split%20in%20tokens%2C%20each%20token%20corresponds%20to%20a%20single%20vector%20of%20weights%20in%20the%20first%20layer%0A%20%20%20%20-%20Embeddings%20are%20trained%20through%20gradient%20descent%2C%20which%20changes%20the%20weights%20to%20minimize%20a%20target%20error%20function%0A%0A%20%20%20%20!%5BEmbeddings%20example%5D(%22public%2Fembeddings_example.png)%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20Bad%20tokenization%20schemes%0A%0A%20%20%20%20-%20Most%20current%20LLMs%20tokenize%20numbers%20in%20an%20unintuitive%20way.%0A%20%20%20%20%20%20%20%20-%20L2R%20%20tokenization%3A%20%24%5Cunderbrace%7B123%7D%20%5C%3B%20%5Cunderbrace%7B456%7D%20%5C%3B%20%5Cunderbrace%7B7%7D%24%0A%20%20%20%20%20%20%20%20-%20R2L%20%20tokenization%3A%20%24%5Cunderbrace%7B1%7D%20%5C%3B%20%5Cunderbrace%7B234%7D%20%5C%3B%20%5Cunderbrace%7B567%7D%24%0A%20%20%20%20-%20R2L%20is%20preferrable%20for%20numeric%20computations%20(think%20doing%20additions%20in%20column)%2C%20so%20why%20do%20LLMs%20use%20L2R%3F%0A%20%20%20%20%20%20%20%20-%20Reason%3A%20BPE%20algorithm%0A%0A%20%20%20%20%5B%F0%9F%94%97%20Tiktokenizer%5D(https%3A%2F%2Ftiktokenizer.vercel.app%2F)%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20BPE%20algorithm%0A%20%20%20%20-%20We%20construct%20a%20list%20of%20tokens%20for%20the%20LLM%20to%20consider%20as%20%22units%22%20to%20work%20with%0A%20%20%20%20%20%20%20%20-%20Start%3A%20each%20singular%20character%20gets%20a%20token%20(%60a%60%2C%20%60b%60%2C%20%60c%60%2C%20...)%0A%20%20%20%20%20%20%20%20-%20Until%20we%20reach%20the%20target%20%60vocabulary_size%60%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20Add%20the%20most%20common%20occurring%20pair%20of%20tokens%20as%20a%20new%20token%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20For%20example%2C%20the%20common%20preposition%20%60to%60%20is%20very%20common%20in%20English%20text%2C%20so%20it%20may%20get%20added%20to%20the%20target%20vocabulary%20(%60a%60%2C%20%60b%60%2C%20%60c%60%2C%20...%2C%20%60to%60)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20After%20some%20cycles%2C%20the%20most%20common%20occorring%20pair%20of%20tokens%20might%20be%20%60to%60%20and%20%60m%60%20to%20form%20the%20common%20name%20%60tom%60%20(%60a%60%2C%20%60b%60%2C%20%60c%60%2C%20...%2C%20%60to%60%2C%20...%2C%20%60tom%60)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20This%20is%20how%20vocabularies%20in%20modern%20LLMs%20are%20built.%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20-%20BPE%20constructs%20its%20vocabulary%20by%20associating%20tokens%20from%20left%20to%20right.%0A%20%20%20%20-%20Tokenizers%20split%20sentences%20according%20to%20the%20vocabulary%20with%20the%20same%20logic%20(L2R).%0A%20%20%20%20-%20GPT-2%20and%20GPT-3%20used%20the%20same%20criteria%20for%20numbers%20as%20it%20did%20for%20words%2C%20leading%20to%20tokenization%20of%20only%20most%20frequently%20occurring%20numbers%20!!!%0A%20%20%20%20-%20Most%20open-source%20LLMs%20today%20always%20tokenize%20integers%20from%200%20to%20999%20using%20a%20single%20token%2C%20but%20they%20still%20group%20tokens%20L2R%20(123%2C456%2C7)%0A%20%20%20%20!%5BGPT-2%20unique%20numeric%20tokens.%20A%20yellow%20square%20means%20the%20corresponding%20number%20is%20represented%20with%20a%20single%20token.%5D(%22public%2Funique_tokens.png%22)%0A%20%20%20%20%5B%F0%9F%94%97%20beren.io%20-%20Integer%20Tokenization%20is%20Insane%5D(https%3A%2F%2Fwww.beren.io%2F2023-02-04-Integer-tokenization-is-insane%2F)%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20Bad%20Clustering%20and%20Inference-time%20correction%0A%0A%20%20%20%20-%20A%20study%20has%20been%20conducted%20to%20benchmark%20addition%20performed%20with%20both%20L2R%20and%20R2L%20clustering%0A%20%20%20%20%20%20%20%20%20%20-%20%F0%9F%A7%AA%20Experiment%3A%20ask%20the%20same%20addition%20problems%20twice%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20First%20time%20neutrally%3A%201234567%20%2B%20654321%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20Second%20time%20clustering%20digits%20with%20commas%3A%201%2C234%2C567%20%2B%20654%2C321%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20Commas%20force%20token%20separation%20at%20the%20point%20of%20insertion%0A%20%20%20%20-%20R2L%20tokenization%20dramatically%20outperforms%20standard%20L2R%20tokenization%0A%20%20%20%20%20%20%20%20-%20GPT-3.5%3A%2075.6%25%20accuracy%20(L2R)%20vs%2097.8%25%20accuracy%20(R2L)%0A%20%20%20%20%20%20%20%20-%20GPT-4%3A%2084.4%25%20accuracy%20(L2R)%20vs%2098.9%25%20accuracy%20(R2L)%0A%0A%0A%20%20%20%20%5B%F0%9F%93%9A%20Tokenization%20counts%3A%20the%20impact%20of%20tokenization%20on%20arithmetic%20in%20frontier%20LLMs%5D(https%3A%2F%2Farxiv.org%2Fabs%2F2402.14903)%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20-%20The%20models%20had%20been%20trained%20with%20a%20L2R%20tokenization%20scheme%2C%20yet%20forcing%20a%20R2L%20one%20improves%20performance%20without%20needing%20to%20train%20the%20model%20again%0A%0A%20%20%20%20-%20%20The%20model%20weights%20appear%20to%20have%20learned%20arithmetic%20algorithms%20that%20are%20fundamentally%20R2L-oriented%2C%20despite%20being%20trained%20predominantly%20on%20L2R%20tokenized%20data.%0A%20%20%20%20%20%20%20%20-%20Inductive%20biases%20emerge%20from%20mathematical%20structure%2C%20not%20just%20data%20statistics%20%3C%3F%3E%0A%0A%20%20%20%20-%20This%20is%20also%20a%20clue%20that%20the%20**underlying%20embedding%20representation%20of%20numbers%20is%20representative%20of%20general%20mathematical%20principles**%2C%20and%20not%20just%20rote%20memorization.%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20The%20Platonic%20Representation%20Hypothesis%0A%0A%20%20%20%20%5B%F0%9F%93%9AHuh%20et%20al.%20(2024)%5D(https%3A%2F%2Farxiv.org%2Fabs%2F2405.07987)%20argue%20that%20all%20models%20are%20converging%20to%20a%20shared%20statistical%20model%20of%20reality.%0A%0A%20%20%20%20-%20**Vision-Vision%20Alignment**%0A%20%20%20%20%20%20%20%20-%20Tested%2078%20vision%20models%20of%20various%20architectures%20and%20training%20objectives%0A%20%20%20%20%20%20%20%20-%20Models%20with%20higher%20performance%20on%20VTAB%20tasks%20showed%20greater%20mutual%20alignment%0A%20%20%20%20%20%20%20%20-%20Competent%20models%20clustered%20together%20in%20representation%20space%0A%0A%20%20%20%20-%20**Cross-Modal%20Alignment**%0A%20%20%20%20%20%20%20%20-%20Vision%20models%20and%20language%20models%20show%20increasing%20alignment%20as%20they%20scale%0A%20%20%20%20%20%20%20%20-%20Models%20can%20be%20%22stitched%22%20together%20across%20modalities%20with%20simple%20learned%20mappings%0A%20%20%20%20%20%20%20%20-%20Color%20representations%20learned%20from%20text%20match%20those%20learned%20from%20images%0A%0A%20%20%20%20-%20**Evidence%20from%20Model%20Stitching**%0A%20%20%20%20%20%20%20%20-%20Different%20models%20can%20have%20their%20intermediate%20layers%20successfully%20swapped%0A%20%20%20%20%20%20%20%20-%20This%20works%20even%20across%20different%20training%20objectives%20and%20datasets%0A%20%20%20%20%20%20%20%20-%20Success%20indicates%20compatible%20internal%20representations%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20To%20recap%3A%0A%0A%20%20%20%20-%20Concrete%20structures%20can%20provide%20Savants%20and%20synaesthetes%20access%20to%20mathematical%20knowledge%0A%20%20%20%20-%20LLMs%20generalize%20over%20mathematical%20concepts%20even%20when%20they're%20trained%20wrongly%0A%20%20%20%20-%20Representations%20in%20LLMs%20seem%20to%20be%20converging%0A%0A%20%20%20%20By%20looking%20into%20the%20structure%20of%20embeddings%2C%20we%20seek%20to%20understand%20the%20shape%20these%20might%20approach.%20To%20understand%20them%2C%20we%20represent%20them%20using%20dimensionality%20reduction%20techniques.%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20Dimensionality%20Reduction%0A%20%20%20%20We%20employ%20the%20most%20commonly%20used%20dimensionality%20reduction%20techniques%3A%0A%0A%20%20%20%20-%20**Linear**%0A%20%20%20%20%20%20%20%20-%20**Singular%20Value%20Decomposition**%20%20%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20decomposes%20a%20matrix%20as%20%24A%20%3D%20U%20%5CSigma%20V%5ET%24%20(rotation%2C%20stretch%2C%20rotation).%20%24%5CSigma%24%20contains%20the%20singular%20values%2C%20which%20**measure%20the%20data%20stretch%20along%20each%20principal%20direction**.%20%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20We%20then%20sort%20the%20features%20by%20their%20singular%20values%20to%20get%20the%20most%20significant%20ones.%0A%0A%20%20%20%20%20%20%20%20-%20**Principal%20Component%20Analysis**%20%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20like%20SVD%2C%20but%20centers%20the%20features%20(subtracts%20the%20mean)%20first.%20As%20a%20consequence%2C%20the%20singular%20values%20correspond%20to%20**the%20standard%20deviations%20along%20each%20principal%20component**.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20has%20cleaner%20statistical%20interpretation%2C%20but%20can%20destroy%20non-centered%20structures%0A%0A%20%20%20%20-%20**Non-Linear**%0A%20%20%20%20%20%20%20%20-%20**t-SNE**%20%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20Models%20proximity%20relationships%20in%20the%20data%20as%20a%20probability%20distribution%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20Recreates%20the%20data%20points%20in%20a%20lower-dimensional%20space%2C%20optimizing%20them%20through%20gradient%20descent%20to%20have%20the%20same%20distribution%0A%20%20%20%20%20%20%20%20-%20**UMAP**%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20preserves%20both%20local%20and%20global%20structure%20using%20topological%20data%20analysis.%20Faster%20than%20t-SNE%20with%20better%20scalability.%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20more%20stable%20results%2C%20better%20preservation%20of%20global%20relationships%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(%0A%20%20%20%20%20%20%20%20r%22%22%22%0A%20%20%20%20%23%20Number%20Semantics%0A%0A%20%20%20%20-%20We%20also%20have%20an%20opportunity%20to%20check%20semantics%20in%20a%20way%20that%20crosses%20the%20symbolic%20barrier.%0A%20%20%20%20-%20We%20can%20relate%20the%20symbols%20themselves%20(0%2C%201%2C%202%2C%20...%2C%20999)%20to%20the%20**numeric%20components%20that%20constitute%20the%20embeddings**%0A%20%20%20%20-%20We%20explore%20this%20idea%20by%20checking%20individual%20features%20for%20all%20the%20integer%20embeddings%20and%20%20their%20correlation%20with%20important%20mathematical%20sequences%3A%0A%20%20%20%20%20%20%20%20-%20%24n_i%20%3D%20i%24%20(the%20numbers%20themselves)%0A%20%20%20%20%20%20%20%20-%20%24n_i%20%3D%20%5Clog(i)%24%0A%20%20%20%20%20%20%20%20-%20Prime%20numbers%0A%20%20%20%20%20%20%20%20-%20Fibonacci%20numbers%0A%20%20%20%20%20%20%20%20-%20Triangular%20numbers%0A%20%20%20%20%22%22%22%0A%20%20%20%20)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20return%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">4b88b92c3ce079fe77a75814afbec17a638253c52eb8085b6012410f98e4eab2</marimo-code-hash>
</body>
</html>
