# Preface {.unnumbered}

This work started with a simple premise: why are LLMs bad at math?

This thesis explores the relationship between numerical reasoning and language model representations, drawing inspiration from savant syndrome to investigate whether concrete representations can bridge the gap between computational capability and mathematical reasoning in large language models.

## Abstract {.unnumbered}

This research investigates the numerical reasoning capabilities of large language models (LLMs) through the lens of embedding analysis. By examining how LLMs represent numerical concepts and exploring parallels with savant cognition, we seek to understand whether language models develop concrete representations that could facilitate mathematical reasoning.

The work combines:

- A literature review on concrete representations in both human cognition and artificial systems
- An empirical analysis of numerical embeddings in current language models, revealing remarkable structural patterns in learned representations

## Structure {.unnumbered}

This thesis is organized as follows:

- **Chapter 1** provides an introduction to the problem and theoretical framework
- **Bibliography** contains all referenced works

The analysis includes extensive visualizations of embedding patterns across different model architectures, including PCA, SVD, t-SNE, and UMAP projections that reveal the geometric structure underlying numerical representations in transformer-based language models.
