# Introduction

This thesis explores LLMs from the perspective of their embeddings, in
particular their numerical ones. There are several reasons why I came to be
interested in this topic, the first and naive one being that tokenization
schemes, as naively implemented with the BPE algorithm, would leave a lot of
space for improvement in numerical tasks, and it's interesting to explore how.

Better performance in LLMs has been sought through the lenses of scale, and
looking for emergent properties as training time and resources increase. There
are different reasons for this, one of them being The Bitter Lesson, a heuristic
principle that states that general methods that better leverage computation are
better than methods that seek to use human domain-specific knowledge to inform
the implementation. This has been observed, for example, in the domain of chess,
where the strategies being put forward hard-coding human domain-specific knowledge
were ultimately beaten by deep search.

After the big success story of scaling in LLMs, the main reach has been towards scale,
and in finding ways to unlock the emergent abilities that would come with them. While
this approach has given results, albeit with some inconsistencies hard to reconcile
from an epistemological perspective, such as the difficulty of actually designing good
benchmarks for those abilities that actually verify they go beyond memorization, this
road would lead to the monopolistic control of the best version of this tool to the
actors that are able to get access to the most amount of data.

If LLM improvement is purely a game of resource accumulation, this could exacerbate the
inequalities we're living under today, leading effectively to a crystallization of the
power structures that, as of today, have the biggest capacity for data collection.
As the state of the art gets better, barriers of entry rise in terms of performance,
training data and hardware required to have a model that performs competitively. As
such, it is of primary importance to find strategies to break through the massive
resource requirements needed for AI training and performance, and find alternative
strategies to allow for the development of the field.

What is investigated here are some possible improvements that don't come
directly from training, but from a better understanding of how, through learning, the
model weights that allow LLMs to function come to be. This comes through thought
experiments about human cognition, and anomalous cases of it (in particular, we'll take
a look at Savant syndrome and what it can tell us in the context of learning), as well
as recent research put forward about LLM capabilities in the specific field of math.

I hope that this can be the beginning of a process through which we are able to see and
work with these systems with a fresh look ... <?>


bitter lesson, focus on inductive bias to allow for greener/more independents
solutions, when is inductive bias too general/specific

tokenization as a process and its obsolescence

representation and explainability as a topic of interest

representation in humans, reification of learning objects


# Background

## The inductive bias of Tokenization

Modern LLMs are built on the Transformer architecture [@vaswani2023], which
operates by converting input text into sequences of discrete tokens that are
then mapped to high-dimensional vector representations. This initial
tokenization step creates an inductive bias that shapes how the model processes
information [@ali2024; @singh2024], with significant implications for the
application of the numerical data to arithmetical tasks.

The most used algorithm for tokenization is currently Byte-Pair Encoding, which,
given a fixed vocabulary size, starts with individual characters and iteratively
merges the most frequently occurring pairs of adjacent tokens until the
vocabulary limit is reached.  This process naturally creates longer tokens for
common substrings that appear frequently in the training data. For numbers, this
means that frequently occurring numerical patterns like "100", "2020", or "999"
might become single tokens, while less common numbers get broken into smaller
pieces. The result is an idiosyncratic and unpredictable tokenization scheme
where similar numbers can be tokenized completely differently based purely on
their frequency in the training corpus. While GPT-2 used to have a purely BPE
tokenizer, the successive iteration of GPT and generally more recent models
either tokenize digits separately (so as $'1234' \rightarrow [1, 2, 3, 4]$), or
tokenize clusters of 3 digits, encompassing the integers in the range 0-999.

![GPT-2 number tokenization. Each row represents 100 numbers, yellow squares
mean that the number is represented by a single token, purple ones by multiple
[@millidgeGpt2]](res/gpt2_unique_tokens.png){#fig-gpt2-tokenization width=500px}

Most of the tokenizers right now do L2R (left-to-right) clustering, meaning that
a number such as $12345$ would be divided in two tokens, $123$ and $45$. It has
been shown [@singh2024] that this kind of clustering leads to a lesser
arithmetic performance, as the grouping doesn't match our positional system's
intuitive structure, where digits in the same positional groups (units, tens,
hundreds) should ideally be processed together for arithmetic operations' sake.

An even more surprising development is that forcing the R2L token clustering of
numbers in models already trained with L2R clustering through the use of commas
in the input (ex. $12,345$) leads to big improvements in arithmetic performance
[@millidge2024].

Despite the model learning representations adapted to work with a L2R token clustering
strategy, forcing a R2L clustering at inference time shows substantial improvements in
arithmetic tasks, which means that despite being learned through an unfavorable
tokenization approach, the numeric
representations retain the properties that allow for the performance to improve when the
clustering scheme is corrected.

There can be different hypotheses on why this might be, for example:

- Arithmetic operations would still work locally in the 0-999 range, which allows for a
  correct reading on them and possible generalization on a larger scale.
- The forced tokenization also happens in the data, as numbers are often separated by
  punctuation in clusters of 3 digits, right to left, for legibility reasons
  [@singh2024]
- A self-correcting emerging learning structure <?>

At the very least, the data being biased towards a R2L representation (in the form of
using the Arabic number system and adopting legibility rules that accommodate
right to left calculations) leads to embeddings that maintain that bias even
when learned in a L2R fashion. This can be a possible hint towards the
optimality of certain representations compared to others, given the resilience
in preferring a certain tokenization scheme over the one the model is trained
on.


| **Model**           | **Strategy**           |
| ------------------- | ---------------------- |
| LLaMA 1 & 2         | Single digit           |
| LLaMA 3             | L2R chunks of 3 digits |
| OLMo 2              | L2R chunks of 3 digits |
| GPT-2               | Pure BPE               |
| GPT-3.5 / GPT-4     | L2R chunks of 3 digits |
| Claude 3 / Claude 4 | R2L chunks of 3 digits |


: Language models with their respective tokenization strategy for numbers.
{#tbl-tokenization}


## Reification as computed embeddings - xVal

There have been other, more comprehensive approaches to the improvement of the
representation of numeric values. xVal is a notable one, as its approach
encompasses real numbers beyond just integers and does away with learning
different representation for each number.

The idea is maximizing the inductive bias in the representation by having
embeddings that are computed based on the number to be represented. Numerical
values represented by a single embedding vector associated with the `[NUM]`
special token.

This fits very well with the idea of reification: the embedding is no longer
just a representation, but it contains and has properties of the object it
represents.

The model uses two separate heads for number and token predictions. If the token
head predicts a `[NUM]` token as the successor, the number head gets activated
and outputs a scalar. The rest of the weights in the transformer blocks are
shared, allowing the learning of representations that are useful for both
discrete text prediction and continuous numerical prediction. This means the
model develops number-aware internal representations throughout all its layers,
not just at the output.  The shared weights force the model to learn features
that work for both linguistic and mathematical reasoning simultaneously.

The approach is shown to improve performance over a series of other techniques,
mostly using a standard notation to represent numbers. This work has been
inspired by the xVal paper, with one of its initial goals being to find good
representations for computed numerical embeddings.

## State of the Art

There are several different approaches to improving math performance in LLMs that
doesn't necessarily come through sheer resizing of the training data.

...



## Background


### Embeddings Structural Approaches <title reviseable, meaning "trying to work with embeddings to improve math performance">

mcleish abacus embeddings
xval


### Dimensionality reduction

PCA, SVD, t-SNE, UMAP

### Cognitive science

murray

### Research on Model Convergence

platonic representation hypothesis

During the process of literature review for this thesis, and after the main
experimantation, we also found <language models use trigonometry to do addition>,
which confirmed some of the visual findings of the work as well as giving ulterior ideas
on which to continue the analysis, especially in reference to the fitting of spirals and
the connection with Fibonacci numbers.

## The search for better suited representation

A case study of savant patient DT [@murray2010] reveals a mathematical cognitive
architecture with the following characteristics:

- Has sequence-space synesthesia with a "mathematical landscape" containing numbers
  0-9999
- Each number possesses specific colors, textures, sizes, and sometimes movements or
  sounds
- Prime numbers have distinctive object properties that distinguish them from other
  numbers
- Arithmetic calculations happen automatically
- solutions appear as part of his visual landscape without conscious effort
- fMRI studies showed that even unstructured number sequences had coherent visual
  structure for DT

Sequence-space synesthesia involves the spontaneous visualization of numerical
sequences in organized spatial arrangements. The remarkable mathematical
abilities of savants with this condition suggest that their specialized
perceptual representations confer significant computational advantages over
conventional symbolic processing.

This raises a compelling question for artificial intelligence: do large language
models spontaneously develop consistent structured representation that make this kind of
calculation easier?

While on one hand we talked about whether the particular mode of cognition of the Savant
can be imitated in explicitly reifing the representation (through the xVal approach),
what the rest of the analysis hinges on is whether LLMs develop such structures on their
own, by looking at their embeddings, as this could hypothethycally inform us on how to
build these structures ourselves in a more direct way than training.




This thesis explores LLMs from the perspective of their embeddings, in
particular their numerical ones. There are several reasons why I came to be
interested in this topic, the first and naive one being that tokenization
schemes, as naively implemented with the BPE algorithm, would leave a lot of
space for improvement in numerical tasks, and it's interesting to explore how.

Better performance in LLMs has been sought through the lenses of scale, and
looking for emergent properties as training time and resources increase. There
are different reasons for this, one of them being The Bitter Lesson, a heuristic
principle that states that general methods that better leverage computation are
better than methods that seek to use human domain-specific knowledge to inform
the implementation. This has been observed, for example, in the domain of chess,
where the strategies being put forward hard-coding human domain-specific knowledge
were ultimately beaten by deep search.

After the big success story of scaling in LLMs, the main reach has been towards scale,
and in finding ways to unlock the emergent abilities that would come with them. While
this approach has given results, albeit with some inconsistencies hard to reconcile
from an epistemological perspective, such as the difficulty of actually designing good
benchmarks for those abilities that actually verify they go beyond memorization, this
road would lead to the monopolistic control of the best version of this tool to the
actors that are able to get access to the most amount of data.

If LLM improvement is purely a game of resource accumulation, this could exacerbate the
inequalities we're living under today, leading effectively to a crystallization of the
power structures that, as of today, have the biggest capacity for data collection.
As the state of the art gets better, barriers of entry rise in terms of performance,
training data and hardware required to have a model that performs competitively. As
such, it is of primary importance to find strategies to break through the massive
resource requirements needed for AI training and performance, and find alternative
strategies to allow for the development of the field.

What is investigated here are some possible improvements that don't come
directly from training, but from a better understanding of how, through learning, the
model weights that allow LLMs to function come to be. This comes through thought
experiments about human cognition, and anomalous cases of it (in particular, we'll take
a look at Savant syndrome and what it can tell us in the context of learning), as well
as recent research put forward about LLM capabilities in the specific field of math.

I hope that this can be the beginning of a process through which we are able to see and
work with these systems with a fresh look ... <?>


bitter lesson, focus on inductive bias to allow for greener/more independents
solutions, when is inductive bias too general/specific

tokenization as a process and its obsolescence

representation and explainability as a topic of interest

representation in humans, reification of learning objects


# Background

## The inductive bias of Tokenization

Modern LLMs are built on the Transformer architecture [@vaswani2023], which
operates by converting input text into sequences of discrete tokens that are
then mapped to high-dimensional vector representations. This initial
tokenization step creates an inductive bias that shapes how the model processes
information [@ali2024; @singh2024], with significant implications for the
application of the numerical data to arithmetical tasks.

The most used algorithm for tokenization is currently Byte-Pair Encoding, which,
given a fixed vocabulary size, starts with individual characters and iteratively
merges the most frequently occurring pairs of adjacent tokens until the
vocabulary limit is reached.  This process naturally creates longer tokens for
common substrings that appear frequently in the training data. For numbers, this
means that frequently occurring numerical patterns like "100", "2020", or "999"
might become single tokens, while less common numbers get broken into smaller
pieces. The result is an idiosyncratic and unpredictable tokenization scheme
where similar numbers can be tokenized completely differently based purely on
their frequency in the training corpus. While GPT-2 used to have a purely BPE
tokenizer, the successive iteration of GPT and generally more recent models
either tokenize digits separately (so as $'1234' \rightarrow [1, 2, 3, 4]$), or
tokenize clusters of 3 digits, encompassing the integers in the range 0-999.

![GPT-2 number tokenization. Each row represents 100 numbers, yellow squares
mean that the number is represented by a single token, purple ones by multiple
[@millidgeGpt2]](res/gpt2_unique_tokens.png){#fig-gpt2-tokenization width=500px}

Most of the tokenizers right now do L2R (left-to-right) clustering, meaning that
a number such as $12345$ would be divided in two tokens, $123$ and $45$. It has
been shown [@singh2024] that this kind of clustering leads to a lesser
arithmetic performance, as the grouping doesn't match our positional system's
intuitive structure, where digits in the same positional groups (units, tens,
hundreds) should ideally be processed together for arithmetic operations' sake.

An even more surprising development is that forcing the R2L token clustering of
numbers in models already trained with L2R clustering through the use of commas
in the input (ex. $12,345$) leads to big improvements in arithmetic performance
[@millidge2024].

Despite the model learning representations adapted to work with a L2R token clustering
strategy, forcing a R2L clustering at inference time shows substantial improvements in
arithmetic tasks, which means that despite being learned through an unfavorable
tokenization approach, the numeric
representations retain the properties that allow for the performance to improve when the
clustering scheme is corrected.

There can be different hypotheses on why this might be, for example:

- Arithmetic operations would still work locally in the 0-999 range, which allows for a
  correct reading on them and possible generalization on a larger scale.
- The forced tokenization also happens in the data, as numbers are often separated by
  punctuation in clusters of 3 digits, right to left, for legibility reasons
  [@singh2024]
- A self-correcting emerging learning structure <?>

At the very least, the data being biased towards a R2L representation (in the form of
using the Arabic number system and adopting legibility rules that accommodate
right to left calculations) leads to embeddings that maintain that bias even
when learned in a L2R fashion. This can be a possible hint towards the
optimality of certain representations compared to others, given the resilience
in preferring a certain tokenization scheme over the one the model is trained
on.


| **Model**           | **Strategy**           |
| ------------------- | ---------------------- |
| LLaMA 1 & 2         | Single digit           |
| LLaMA 3             | L2R chunks of 3 digits |
| OLMo 2              | L2R chunks of 3 digits |
| GPT-2               | Pure BPE               |
| GPT-3.5 / GPT-4     | L2R chunks of 3 digits |
| Claude 3 / Claude 4 | R2L chunks of 3 digits |


: Language models with their respective tokenization strategy for numbers.
{#tbl-tokenization}

## Reification as computed embeddings - xVal

There have been other, more comprehensive approaches to the improvement of the
representation of numeric values. xVal is a notable one, as its approach
encompasses real numbers beyond just integers and does away with learning
different representation for each number.

The idea is maximizing the inductive bias in the representation by having
embeddings that are computed based on the number to be represented. Numerical
values represented by a single embedding vector associated with the `[NUM]`
special token.

This fits very well with the idea of reification: the embedding is no longer
just a representation, but it contains and has properties of the object it
represents.

The model uses two separate heads for number and token predictions. If the token
head predicts a `[NUM]` token as the successor, the number head gets activated
and outputs a scalar. The rest of the weights in the transformer blocks are
shared, allowing the learning of representations that are useful for both
discrete text prediction and continuous numerical prediction. This means the
model develops number-aware internal representations throughout all its layers,
not just at the output.  The shared weights force the model to learn features
that work for both linguistic and mathematical reasoning simultaneously.

The approach is shown to improve performance over a series of other techniques,
mostly using a standard notation to represent numbers. This work has been
inspired by the xVal paper, with one of its initial goals being to find good
representations for computed numerical embeddings.

## The search for better suited representation

A case study of savant patient DT [@murray2010] reveals a mathematical cognitive
architecture with the following characteristics:

- Has sequence-space synesthesia with a "mathematical landscape" containing numbers
  0-9999
- Each number possesses specific colors, textures, sizes, and sometimes movements or
  sounds
- Prime numbers have distinctive object properties that distinguish them from other
  numbers
- Arithmetic calculations happen automatically
- solutions appear as part of his visual landscape without conscious effort
- fMRI studies showed that even unstructured number sequences had coherent visual
  structure for DT

Sequence-space synesthesia involves the spontaneous visualization of numerical
sequences in organized spatial arrangements. The remarkable mathematical
abilities of savants with this condition suggest that their specialized
perceptual representations confer significant computational advantages over
conventional symbolic processing.

This raises a compelling question for artificial intelligence: do large language
models spontaneously develop consistent structured representation that make this kind of
calculation easier?

While on one hand we talked about whether the particular mode of cognition of the Savant
can be imitated in explicitly reifing the representation (through the xVal approach),
what the rest of the analysis hinges on is whether LLMs develop such structures on their
own, by looking at their embeddings, as this could hypothethycally inform us on how to
build these structures ourselves in a more direct way than training.


