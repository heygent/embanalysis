@online{ali2024,
  title = {Tokenizer {{Choice For LLM Training}}: {{Negligible}} or {{Crucial}}?},
  shorttitle = {Tokenizer {{Choice For LLM Training}}},
  author = {Ali, Mehdi and Fromm, Michael and Thellmann, Klaudia and Rutmann, Richard and Lübbering, Max and Leveling, Johannes and Klug, Katrin and Ebert, Jan and Doll, Niclas and Buschhoff, Jasper Schulze and Jain, Charvi and Weber, Alexander Arno and Jurkschat, Lena and Abdelwahab, Hammam and John, Chelsea and Suarez, Pedro Ortiz and Ostendorff, Malte and Weinbach, Samuel and Sifa, Rafet and Kesselheim, Stefan and Flores-Herr, Nicolas},
  date = {2024-03-17},
  eprint = {2310.08754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08754},
  url = {http://arxiv.org/abs/2310.08754},
  urldate = {2025-06-25},
  abstract = {The recent success of Large Language Models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68\%, due to an inefficient tokenization vocabulary.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/NT3WMWYQ/Ali et al. - 2024 - Tokenizer Choice For LLM Training Negligible or Crucial.pdf;/Users/gent/Zotero/storage/SFB8E8N6/2310.html}
}

@article{cowan2009,
  title = {Do Calendrical Savants Use Calculation to Answer Date Questions? {{A}} Functional Magnetic Resonance Imaging Study},
  shorttitle = {Do Calendrical Savants Use Calculation to Answer Date Questions?},
  author = {Cowan, Richard and Frith, Chris},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  number = {1522},
  eprint = {19528025},
  eprinttype = {pubmed},
  pages = {1417--1424},
  issn = {0962-8436},
  doi = {10.1098/rstb.2008.0323},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677581/},
  urldate = {2025-06-25},
  abstract = {Calendrical savants can name the weekdays for dates from different years with remarkable speed and accuracy. Whether calculation rather than just memory is involved is disputed. Grounds for doubting whether they can calculate are reviewed and criteria for attributing date calculation skills to them are discussed. At least some calendrical savants possess date calculation skills. A behavioural characteristic observed in many calendrical savants is increased response time for questions about more remote years. This may be because more remote years require more calculation or because closer years are more practised. An experiment is reported that used functional magnetic resonance imaging to attempt to discriminate between these explanations. Only two savants could be scanned and excessive head movement corrupted one savant's mental arithmetic data. Nevertheless, there was increased parietal activation during both mental arithmetic and date questions and this region showed increased activity with more remote dates. These results suggest that the calendrical skills observed in savants result from intensive practice with calculations used in solving mental arithmetic problems. The mystery is not how they solve these problems, but why.},
  pmcid = {PMC2677581},
  file = {/Users/gent/Zotero/storage/WYDBH6BH/Cowan e Frith - 2009 - Do calendrical savants use calculation to answer date questions A functional magnetic resonance ima.pdf}
}

@online{golkar2023,
  title = {{{xVal}}: {{A Continuous Number Encoding}} for {{Large Language Models}}},
  shorttitle = {{{xVal}}},
  author = {Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and Blancard, Bruno Régaldo-Saint and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  date = {2023-10-04},
  eprint = {2310.02989},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.02989},
  url = {http://arxiv.org/abs/2310.02989},
  urldate = {2024-10-21},
  abstract = {Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/E96E7D4C/Golkar et al. - 2023 - xVal A Continuous Number Encoding for Large Language Models.pdf;/Users/gent/Zotero/storage/BUSKUXST/2310.html}
}

@online{huh2024,
  title = {The {{Platonic Representation Hypothesis}}},
  author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  date = {2024-05-13},
  eprint = {2405.07987},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.07987},
  url = {http://arxiv.org/abs/2405.07987},
  urldate = {2024-06-13},
  abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found},
  file = {/Users/gent/Zotero/storage/WVQ3UQUT/Huh et al. - 2024 - The Platonic Representation Hypothesis.pdf;/Users/gent/Zotero/storage/U2RT699X/2405.html}
}

@online{mcleish2024,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  date = {2024-12-23},
  eprint = {2405.17399},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.17399},
  url = {http://arxiv.org/abs/2405.17399},
  urldate = {2025-06-25},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/UVYXN3Q5/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf;/Users/gent/Zotero/storage/B73ZEVGC/2405.html}
}

@article{mottron2006,
  title = {Non-Algorithmic Access to Calendar Information in a Calendar Calculator with Autism},
  author = {Mottron, L. and Lemmens, K. and Gagnon, L. and Seron, X.},
  date = {2006-02},
  journaltitle = {Journal of Autism and Developmental Disorders},
  shortjournal = {J Autism Dev Disord},
  volume = {36},
  number = {2},
  eprint = {16453069},
  eprinttype = {pubmed},
  pages = {239--247},
  issn = {0162-3257},
  doi = {10.1007/s10803-005-0059-9},
  abstract = {The possible use of a calendar algorithm was assessed in DBC, an autistic "savant" of normal measured intelligence. Testing of all the dates in a year revealed a random distribution of errors. Re-testing DBC on the same dates one year later shows that his errors were not stable across time. Finally, DBC was able to answer "reversed" questions that cannot be solved by a classical algorithm. These findings favor a non-algorithmic retrieval of calendar information. It is proposed that multidirectional, non-hierarchical retrieval of information, and solving problems in a non-algorithmic way, are involved in savant performances. The possible role of a functional rededication of low-level perceptual systems to the processing of symbolic information in savants is discussed.},
  langid = {english},
  keywords = {Adolescent,Algorithms,Aptitude,Autistic Disorder,Humans,Male,Reaction Time,Time Perception},
  file = {/Users/gent/Zotero/storage/6BL2CJZE/Mottron et al. - 2006 - Non-algorithmic access to calendar information in a calendar calculator with autism.pdf}
}

@article{murray2010,
  title = {Can the Existence of Highly Accessible Concrete Representations Explain Savant Skills? {{Some}} Insights from Synaesthesia},
  shorttitle = {Can the Existence of Highly Accessible Concrete Representations Explain Savant Skills?},
  author = {Murray, A. Louise},
  date = {2010-06},
  journaltitle = {Medical Hypotheses},
  shortjournal = {Medical Hypotheses},
  volume = {74},
  number = {6},
  pages = {1006--1012},
  issn = {03069877},
  doi = {10.1016/j.mehy.2010.01.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306987710000186},
  urldate = {2024-09-22},
  abstract = {Semantic Scholar extracted view of "Can the existence of highly accessible concrete representations explain savant skills? Some insights from synaesthesia." by A. Murray},
  langid = {english},
  file = {/Users/gent/Zotero/storage/HHKLR3Q2/Murray - 2010 - Can the existence of highly accessible concrete representations explain savant skills Some insights.pdf}
}

@online{singh2024,
  title = {Tokenization Counts: The Impact of Tokenization on Arithmetic in Frontier {{LLMs}}},
  shorttitle = {Tokenization Counts},
  author = {Singh, Aaditya K. and Strouse, D. J.},
  date = {2024-02-22},
  eprint = {2402.14903},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.14903},
  url = {http://arxiv.org/abs/2402.14903},
  urldate = {2025-06-25},
  abstract = {Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/TI2NHPKS/Singh e Strouse - 2024 - Tokenization counts the impact of tokenization on arithmetic in frontier LLMs.pdf;/Users/gent/Zotero/storage/G9UQGNY6/2402.html}
}

@article{snyder2009,
  title = {Explaining and Inducing Savant Skills: Privileged Access to Lower Level, Less-Processed Information},
  shorttitle = {Explaining and Inducing Savant Skills},
  author = {Snyder, Allan},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  number = {1522},
  eprint = {19528023},
  eprinttype = {pubmed},
  pages = {1399--1405},
  issn = {0962-8436},
  doi = {10.1098/rstb.2008.0290},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677578/},
  urldate = {2024-09-22},
  abstract = {I argue that savant skills are latent in us all. My hypothesis is that savants have privileged access to lower level, less-processed information, before it is packaged into holistic concepts and meaningful labels. Owing to a failure in top-down inhibition, they can tap into information that exists in all of our brains, but is normally beyond conscious awareness. This suggests why savant skills might arise spontaneously in otherwise normal people, and why such skills might be artificially induced by low-frequency repetitive transcranial magnetic stimulation. It also suggests why autistic savants are atypically literal with a tendency to concentrate more on the parts than on the whole and why this offers advantages for particular classes of problem solving, such as those that necessitate breaking cognitive mindsets. A strategy of building from the parts to the whole could form the basis for the so-called autistic genius. Unlike the healthy mind, which has inbuilt expectations of the world (internal order), the autistic mind must simplify the world by adopting strict routines (external order).},
  pmcid = {PMC2677578},
  file = {/Users/gent/Zotero/storage/278ZIAEC/Snyder - 2009 - Explaining and inducing savant skills privileged access to lower level, less-processed information.pdf}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-02-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/3R8CEW3R/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/gent/Zotero/storage/ECKS6RGX/1706.html}
}

@online{zotero-item-789,
  title = {The {{Bitter Lesson}} Is Coming for {{Tokenization}} | ⛰️ Lucalp},
  url = {https://lucalp.dev/bitter-lesson-tokenization-and-blt/},
  urldate = {2025-06-25},
  file = {/Users/gent/Zotero/storage/ZNTCCEGX/bitter-lesson-tokenization-and-blt.html}
}
