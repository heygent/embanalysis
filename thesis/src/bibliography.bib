@online{ali2024,
  title = {Tokenizer {{Choice For LLM Training}}: {{Negligible}} or {{Crucial}}?},
  shorttitle = {Tokenizer {{Choice For LLM Training}}},
  author = {Ali, Mehdi and Fromm, Michael and Thellmann, Klaudia and Rutmann, Richard and Lübbering, Max and Leveling, Johannes and Klug, Katrin and Ebert, Jan and Doll, Niclas and Buschhoff, Jasper Schulze and Jain, Charvi and Weber, Alexander Arno and Jurkschat, Lena and Abdelwahab, Hammam and John, Chelsea and Suarez, Pedro Ortiz and Ostendorff, Malte and Weinbach, Samuel and Sifa, Rafet and Kesselheim, Stefan and Flores-Herr, Nicolas},
  date = {2024-03-17},
  eprint = {2310.08754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08754},
  url = {http://arxiv.org/abs/2310.08754},
  urldate = {2025-06-25},
  abstract = {The recent success of Large Language Models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68\%, due to an inefficient tokenization vocabulary.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/NT3WMWYQ/Ali et al. - 2024 - Tokenizer Choice For LLM Training Negligible or Crucial.pdf;/Users/gent/Zotero/storage/SFB8E8N6/2310.html}
}

@online{bitter-lesson,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Rich},
  date = {2019-03-13},
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2025-07-05},
  file = {/Users/gent/Zotero/storage/LEDLVL9S/BitterLesson.html}
}

@online{bitter-lesson-tokenization,
  title = {The {{Bitter Lesson}} Is Coming for {{Tokenization}} | ⛰️ Lucalp},
  author = {{lucalp}},
  date = {2025-06-24},
  url = {https://lucalp.dev/bitter-lesson-tokenization-and-blt/},
  urldate = {2025-06-25},
  file = {/Users/gent/Zotero/storage/ZNTCCEGX/bitter-lesson-tokenization-and-blt.html}
}

@article{cowan2009,
  title = {Do Calendrical Savants Use Calculation to Answer Date Questions? {{A}} Functional Magnetic Resonance Imaging Study},
  shorttitle = {Do Calendrical Savants Use Calculation to Answer Date Questions?},
  author = {Cowan, Richard and Frith, Chris},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  number = {1522},
  eprint = {19528025},
  eprinttype = {pubmed},
  pages = {1417--1424},
  issn = {0962-8436},
  doi = {10.1098/rstb.2008.0323},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677581/},
  urldate = {2025-06-25},
  abstract = {Calendrical savants can name the weekdays for dates from different years with remarkable speed and accuracy. Whether calculation rather than just memory is involved is disputed. Grounds for doubting whether they can calculate are reviewed and criteria for attributing date calculation skills to them are discussed. At least some calendrical savants possess date calculation skills. A behavioural characteristic observed in many calendrical savants is increased response time for questions about more remote years. This may be because more remote years require more calculation or because closer years are more practised. An experiment is reported that used functional magnetic resonance imaging to attempt to discriminate between these explanations. Only two savants could be scanned and excessive head movement corrupted one savant's mental arithmetic data. Nevertheless, there was increased parietal activation during both mental arithmetic and date questions and this region showed increased activity with more remote dates. These results suggest that the calendrical skills observed in savants result from intensive practice with calculations used in solving mental arithmetic problems. The mystery is not how they solve these problems, but why.},
  pmcid = {PMC2677581},
  file = {/Users/gent/Zotero/storage/WYDBH6BH/Cowan e Frith - 2009 - Do calendrical savants use calculation to answer date questions A functional magnetic resonance ima.pdf}
}

@online{deepseek-ai2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  date = {2025-01-22},
  eprint = {2501.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.12948},
  url = {http://arxiv.org/abs/2501.12948},
  urldate = {2025-07-06},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/TINH5MZG/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf;/Users/gent/Zotero/storage/LFN3TKLM/2501.html}
}

@online{golkar2023,
  title = {{{xVal}}: {{A Continuous Number Encoding}} for {{Large Language Models}}},
  shorttitle = {{{xVal}}},
  author = {Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and Blancard, Bruno Régaldo-Saint and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  date = {2023-10-04},
  eprint = {2310.02989},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.02989},
  url = {http://arxiv.org/abs/2310.02989},
  urldate = {2024-10-21},
  abstract = {Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/E96E7D4C/Golkar et al. - 2023 - xVal A Continuous Number Encoding for Large Language Models.pdf;/Users/gent/Zotero/storage/BUSKUXST/2310.html}
}

@online{grattafiori2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and family=Linde, given=Jelmer, prefix=van der, useprefix=false and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and family=Maaten, given=Laurens, prefix=van der, useprefix=false and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and family=Oliveira, given=Luke, prefix=de, useprefix=false and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  date = {2024-11-23},
  eprint = {2407.21783},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.21783},
  url = {http://arxiv.org/abs/2407.21783},
  urldate = {2025-06-26},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/gent/Zotero/storage/USSEYCDV/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf;/Users/gent/Zotero/storage/JNKFAH48/2407.html}
}

@online{huh2024,
  title = {The {{Platonic Representation Hypothesis}}},
  author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  date = {2024-05-13},
  eprint = {2405.07987},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.07987},
  url = {http://arxiv.org/abs/2405.07987},
  urldate = {2024-06-13},
  abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found},
  file = {/Users/gent/Zotero/storage/WVQ3UQUT/Huh et al. - 2024 - The Platonic Representation Hypothesis.pdf;/Users/gent/Zotero/storage/U2RT699X/2405.html}
}

@online{islam2022,
  title = {A {{Vocabulary-Free Multilingual Neural Tokenizer}} for {{End-to-End Task Learning}}},
  author = {Islam, Md Mofijul and Aguilar, Gustavo and Ponnusamy, Pragaash and Mathialagan, Clint Solomon and Ma, Chengyuan and Guo, Chenlei},
  date = {2022-04-22},
  eprint = {2204.10815},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2204.10815},
  url = {http://arxiv.org/abs/2204.10815},
  urldate = {2024-10-24},
  abstract = {Subword tokenization is a commonly used input pre-processing step in most recent NLP models. However, it limits the models' ability to leverage end-to-end task learning. Its frequency-based vocabulary creation compromises tokenization in low-resource languages, leading models to produce suboptimal representations. Additionally, the dependency on a fixed vocabulary limits the subword models' adaptability across languages and domains. In this work, we propose a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization. We pre-train our character-based tokenizer by processing unique words from multilingual corpus, thereby extensively increasing word diversity across languages. Unlike the predefined and fixed vocabularies in subword methods, our tokenizer allows end-to-end task learning, resulting in optimal task-specific tokenization. The experimental results show that replacing the subword tokenizer with our neural tokenizer consistently improves performance on multilingual (NLI) and code-switching (sentiment analysis) tasks, with larger gains in low-resource languages. Additionally, our neural tokenizer exhibits a robust performance on downstream tasks when adversarial noise is present (typos and misspelling), further increasing the initial improvements over statistical subword tokenizers.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/RZLXHSMK/Islam et al. - 2022 - A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning.pdf;/Users/gent/Zotero/storage/8UDTGGKP/2204.html}
}

@online{kantamneni2025,
  title = {Language {{Models Use Trigonometry}} to {{Do Addition}}},
  author = {Kantamneni, Subhash and Tegmark, Max},
  date = {2025-02-02},
  eprint = {2502.00873},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.00873},
  url = {http://arxiv.org/abs/2502.00873},
  urldate = {2025-07-04},
  abstract = {Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the "Clock" algorithm: to solve \$a+b\$, the helices for \$a\$ and \$b\$ are manipulated to produce the \$a+b\$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/5I7U5VM9/Kantamneni e Tegmark - 2025 - Language Models Use Trigonometry to Do Addition.pdf;/Users/gent/Zotero/storage/294SKCSL/2502.html}
}

@article{maaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {family=Maaten, given=Laurens, prefix=van der, useprefix=false and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  urldate = {2025-07-06},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/Users/gent/Zotero/storage/QG34ECGS/Maaten e Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@online{mcinnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-18},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1802.03426},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2025-07-06},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gent/Zotero/storage/N7WYBCF4/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf;/Users/gent/Zotero/storage/7CE85PZX/1802.html}
}

@online{mcleish2024,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  date = {2024-12-23},
  eprint = {2405.17399},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.17399},
  url = {http://arxiv.org/abs/2405.17399},
  urldate = {2025-06-25},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/UVYXN3Q5/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf;/Users/gent/Zotero/storage/B73ZEVGC/2405.html}
}

@online{millidge2023,
  title = {Integer Tokenization Is Insane},
  author = {Millidge, Beren},
  date = {2023-02-04},
  url = {http://www.beren.io/2023-02-04-Integer-tokenization-is-insane/},
  urldate = {2025-06-26},
  abstract = {After spending a lot of time with language models, I have come to the conclusion that tokenization in general is insane and it is a miracle that language models learn anything at all. To drill down into one specific example of silliness which has been bothering me recently, let’s look...},
  langid = {english},
  file = {/Users/gent/Zotero/storage/MAPC448V/2023-02-04-Integer-tokenization-is-insane.html}
}

@online{millidge2024,
  title = {Right to {{Left}} ({{R2L}}) {{Integer Tokenization}}},
  author = {Millidge, Beren},
  date = {2024-07-07},
  url = {http://www.beren.io/2024-07-07-Right-to-Left-Integer-Tokenization/},
  urldate = {2025-06-26},
  abstract = {This is a guest post by Max Buckley, a software engineer at Google and fellow AI researcher1. By some twist of fate, this blog has become the chronicle of the evolution of integer tokenization. In an earlier post in February of 2023, it was discussed how older models: GPT-2 and...},
  langid = {english},
  file = {/Users/gent/Zotero/storage/P9KYXJ22/2024-07-07-Right-to-Left-Integer-Tokenization.html}
}

@article{mottron2006,
  title = {Non-Algorithmic Access to Calendar Information in a Calendar Calculator with Autism},
  author = {Mottron, L. and Lemmens, K. and Gagnon, L. and Seron, X.},
  date = {2006-02},
  journaltitle = {Journal of Autism and Developmental Disorders},
  shortjournal = {J Autism Dev Disord},
  volume = {36},
  number = {2},
  eprint = {16453069},
  eprinttype = {pubmed},
  pages = {239--247},
  issn = {0162-3257},
  doi = {10.1007/s10803-005-0059-9},
  abstract = {The possible use of a calendar algorithm was assessed in DBC, an autistic "savant" of normal measured intelligence. Testing of all the dates in a year revealed a random distribution of errors. Re-testing DBC on the same dates one year later shows that his errors were not stable across time. Finally, DBC was able to answer "reversed" questions that cannot be solved by a classical algorithm. These findings favor a non-algorithmic retrieval of calendar information. It is proposed that multidirectional, non-hierarchical retrieval of information, and solving problems in a non-algorithmic way, are involved in savant performances. The possible role of a functional rededication of low-level perceptual systems to the processing of symbolic information in savants is discussed.},
  langid = {english},
  keywords = {Adolescent,Algorithms,Aptitude,Autistic Disorder,Humans,Male,Reaction Time,Time Perception},
  file = {/Users/gent/Zotero/storage/6BL2CJZE/Mottron et al. - 2006 - Non-algorithmic access to calendar information in a calendar calculator with autism.pdf}
}

@article{murray2010,
  title = {Can the Existence of Highly Accessible Concrete Representations Explain Savant Skills? {{Some}} Insights from Synaesthesia},
  shorttitle = {Can the Existence of Highly Accessible Concrete Representations Explain Savant Skills?},
  author = {Murray, A. Louise},
  date = {2010-06},
  journaltitle = {Medical Hypotheses},
  shortjournal = {Medical Hypotheses},
  volume = {74},
  number = {6},
  pages = {1006--1012},
  issn = {03069877},
  doi = {10.1016/j.mehy.2010.01.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306987710000186},
  urldate = {2024-09-22},
  abstract = {Semantic Scholar extracted view of "Can the existence of highly accessible concrete representations explain savant skills? Some insights from synaesthesia." by A. Murray},
  langid = {english},
  file = {/Users/gent/Zotero/storage/HHKLR3Q2/Murray - 2010 - Can the existence of highly accessible concrete representations explain savant skills Some insights.pdf}
}

@online{olmo2025,
  title = {2 {{OLMo}} 2 {{Furious}}},
  author = {OLMo, Team and Walsh, Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and Lambert, Nathan and Schwenk, Dustin and Tafjord, Oyvind and Anderson, Taira and Atkinson, David and Brahman, Faeze and Clark, Christopher and Dasigi, Pradeep and Dziri, Nouha and Guerquin, Michal and Ivison, Hamish and Koh, Pang Wei and Liu, Jiacheng and Malik, Saumya and Merrill, William and Miranda, Lester James V. and Morrison, Jacob and Murray, Tyler and Nam, Crystal and Pyatkin, Valentina and Rangapur, Aman and Schmitz, Michael and Skjonsberg, Sam and Wadden, David and Wilhelm, Christopher and Wilson, Michael and Zettlemoyer, Luke and Farhadi, Ali and Smith, Noah A. and Hajishirzi, Hannaneh},
  date = {2025-01-15},
  eprint = {2501.00656},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.00656},
  url = {http://arxiv.org/abs/2501.00656},
  urldate = {2025-03-18},
  abstract = {We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\textbackslash "ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/WTMH7WE9/OLMo et al. - 2025 - 2 OLMo 2 Furious.pdf;/Users/gent/Zotero/storage/I99YEJN9/2501.html}
}

@online{pagnoni2024,
  title = {Byte {{Latent Transformer}}: {{Patches Scale Better Than Tokens}}},
  shorttitle = {Byte {{Latent Transformer}}},
  author = {Pagnoni, Artidoro and Pasunuru, Ram and Rodriguez, Pedro and Nguyen, John and Muller, Benjamin and Li, Margaret and Zhou, Chunting and Yu, Lili and Weston, Jason and Zettlemoyer, Luke and Ghosh, Gargi and Lewis, Mike and Holtzman, Ari and Iyer, Srinivasan},
  date = {2024-12-13},
  eprint = {2412.09871},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.09871},
  url = {http://arxiv.org/abs/2412.09871},
  urldate = {2025-07-06},
  abstract = {We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/gent/Zotero/storage/KM6AN8GL/Pagnoni et al. - 2024 - Byte Latent Transformer Patches Scale Better Than Tokens.pdf;/Users/gent/Zotero/storage/596QIC39/2412.html}
}

@inproceedings{radford2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeff and Child, R. and Luan, D. and Amodei, Dario and Sutskever, I.},
  date = {2019},
  url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate = {2025-07-07},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}
}

@artwork{romain2022,
  title = {English:  {{Spiral}} of Fibonacci Number over Tiled Squares},
  shorttitle = {English},
  author = {{Romain}},
  date = {2022-01-18},
  url = {https://commons.wikimedia.org/wiki/File:Fibonacci_Spiral.svg#Licensing},
  urldate = {2025-07-08}
}

@online{singh2024,
  title = {Tokenization Counts: The Impact of Tokenization on Arithmetic in Frontier {{LLMs}}},
  shorttitle = {Tokenization Counts},
  author = {Singh, Aaditya K. and Strouse, D. J.},
  date = {2024-02-22},
  eprint = {2402.14903},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.14903},
  url = {http://arxiv.org/abs/2402.14903},
  urldate = {2025-06-25},
  abstract = {Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gent/Zotero/storage/TI2NHPKS/Singh e Strouse - 2024 - Tokenization counts the impact of tokenization on arithmetic in frontier LLMs.pdf;/Users/gent/Zotero/storage/G9UQGNY6/2402.html}
}

@article{skalse2023,
  title = {Some {{Arguments Against Strong Scaling}}},
  author = {Skalse, Joar},
  date = {2023-01-13},
  url = {https://www.lesswrong.com/posts/DvCLEkr9pXLnWikB8/some-arguments-against-strong-scaling},
  urldate = {2025-07-06},
  abstract = {There are many people who believe that we will be able to get to AGI by basically just scaling up the techniques used in recent large language models…},
  langid = {english},
  file = {/Users/gent/Zotero/storage/5GFLBQKU/some-arguments-against-strong-scaling.html}
}

@article{snyder2009,
  title = {Explaining and Inducing Savant Skills: Privileged Access to Lower Level, Less-Processed Information},
  shorttitle = {Explaining and Inducing Savant Skills},
  author = {Snyder, Allan},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  number = {1522},
  eprint = {19528023},
  eprinttype = {pubmed},
  pages = {1399--1405},
  issn = {0962-8436},
  doi = {10.1098/rstb.2008.0290},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677578/},
  urldate = {2024-09-22},
  abstract = {I argue that savant skills are latent in us all. My hypothesis is that savants have privileged access to lower level, less-processed information, before it is packaged into holistic concepts and meaningful labels. Owing to a failure in top-down inhibition, they can tap into information that exists in all of our brains, but is normally beyond conscious awareness. This suggests why savant skills might arise spontaneously in otherwise normal people, and why such skills might be artificially induced by low-frequency repetitive transcranial magnetic stimulation. It also suggests why autistic savants are atypically literal with a tendency to concentrate more on the parts than on the whole and why this offers advantages for particular classes of problem solving, such as those that necessitate breaking cognitive mindsets. A strategy of building from the parts to the whole could form the basis for the so-called autistic genius. Unlike the healthy mind, which has inbuilt expectations of the world (internal order), the autistic mind must simplify the world by adopting strict routines (external order).},
  pmcid = {PMC2677578},
  file = {/Users/gent/Zotero/storage/278ZIAEC/Snyder - 2009 - Explaining and inducing savant skills privileged access to lower level, less-processed information.pdf}
}

@online{tiktokenizer,
  title = {Tiktokenizer},
  url = {https://tiktokenizer.vercel.app/},
  urldate = {2025-07-08},
  file = {/Users/gent/Zotero/storage/FB8LH5AC/tiktokenizer.vercel.app.html}
}

@online{trask2018,
  title = {Neural {{Arithmetic Logic Units}}},
  author = {Trask, Andrew and Hill, Felix and Reed, Scott and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  date = {2018-08-01},
  eprint = {1808.00508},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1808.00508},
  url = {http://arxiv.org/abs/1808.00508},
  urldate = {2024-09-20},
  abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/8SRYNAQW/Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf;/Users/gent/Zotero/storage/5IJYAW26/1808.html}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-02-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  file = {/Users/gent/Zotero/storage/3R8CEW3R/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/gent/Zotero/storage/ECKS6RGX/1706.html}
}
